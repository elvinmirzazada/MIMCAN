{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Python Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T15:34:36.155927078Z",
     "start_time": "2023-04-30T15:34:33.594339124Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torchvision, transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T08:43:33.235083789Z",
     "start_time": "2023-04-30T08:43:31.870925661Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageFile, UnidentifiedImageError\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, cohen_kappa_score\n",
    "from textwrap import wrap\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from transformers import AdamW, BertModel, BertTokenizer, DistilBertModel\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T15:34:37.120862758Z",
     "start_time": "2023-04-30T15:34:36.159670422Z"
    }
   },
   "source": [
    "### Reading the dataset"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset after dropping not news url containing rows\n",
      "Fake dataset info: \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5067 entries, 0 to 5322\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         5067 non-null   object\n",
      " 1   news_url   5067 non-null   object\n",
      " 2   title      5067 non-null   object\n",
      " 3   tweet_ids  4898 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 197.9+ KB\n",
      "None\n",
      "Real dataset info: \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 16804 entries, 0 to 16816\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         16804 non-null  object\n",
      " 1   news_url   16804 non-null  object\n",
      " 2   title      16804 non-null  object\n",
      " 3   tweet_ids  15747 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 656.4+ KB\n",
      "None\n",
      "Final dataset description:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 21871 entries, 3855 to 14116\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         21871 non-null  object\n",
      " 1   news_url   21871 non-null  object\n",
      " 2   title      21871 non-null  object\n",
      " 3   tweet_ids  20645 non-null  object\n",
      " 4   labels     21871 non-null  int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 1.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T15:34:37.121372319Z",
     "start_time": "2023-04-30T15:34:37.097802479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3549 entries, 13338 to 14116\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         3549 non-null   object\n",
      " 1   news_url   3549 non-null   object\n",
      " 2   title      3549 non-null   object\n",
      " 3   tweet_ids  3345 non-null   object\n",
      " 4   labels     3549 non-null   int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 166.4+ KB\n"
     ]
    }
   ],
   "source": [
    "gossicop_fake = pd.read_csv('datasets/gossipcop_fake.csv')\n",
    "gossicop_real = pd.read_csv('datasets/gossipcop_real.csv')\n",
    "politifact_fake = pd.read_csv('datasets/politifact_fake.csv')\n",
    "politifact_real = pd.read_csv('datasets/politifact_real.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T15:34:37.122315593Z",
     "start_time": "2023-04-30T15:34:37.107431543Z"
    }
   },
   "source": [
    "#### Adding labels to the dataset"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": "                         id  \\\n13338      gossipcop-943464   \n12037      gossipcop-846889   \n17171  gossipcop-2870728999   \n2964       gossipcop-895224   \n17399  gossipcop-1770549032   \n\n                                                news_url  \\\n13338  https://people.com/movies/justin-theroux-emma-...   \n12037  https://people.com/celebrity/busy-philipps-mon...   \n17171  people.com/health/celebrities-told-to-lose-weight   \n2964   https://ew.com/movies/2017/11/20/armie-hammer-...   \n17399  www.inquisitr.com/3101499/justin-bieber-blames...   \n\n                                                   title  \\\n13338  Justin Theroux Seen with Emma Stone Again as T...   \n12037  Busy Philipps Says She Made More Money Doing B...   \n17171  13 Celebrities Who Were Told to Lose Weight (Y...   \n2964   Armie Hammer calls out double standard between...   \n17399  Justin Bieber Blames Taylor Swift For Selena G...   \n\n                                               tweet_ids  labels  \n13338  1006670337285722112\\t1006671444632330240\\t1006...       0  \n12037  858043435953115136\\t858044173865418752\\t858044...       0  \n17171                                                NaN       1  \n2964   932740353605505025\\t932742389332152320\\t932742...       0  \n17399  732205554513379328\\t732224787351621632\\t732225...       1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>news_url</th>\n      <th>title</th>\n      <th>tweet_ids</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>13338</th>\n      <td>gossipcop-943464</td>\n      <td>https://people.com/movies/justin-theroux-emma-...</td>\n      <td>Justin Theroux Seen with Emma Stone Again as T...</td>\n      <td>1006670337285722112\\t1006671444632330240\\t1006...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12037</th>\n      <td>gossipcop-846889</td>\n      <td>https://people.com/celebrity/busy-philipps-mon...</td>\n      <td>Busy Philipps Says She Made More Money Doing B...</td>\n      <td>858043435953115136\\t858044173865418752\\t858044...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17171</th>\n      <td>gossipcop-2870728999</td>\n      <td>people.com/health/celebrities-told-to-lose-weight</td>\n      <td>13 Celebrities Who Were Told to Lose Weight (Y...</td>\n      <td>NaN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2964</th>\n      <td>gossipcop-895224</td>\n      <td>https://ew.com/movies/2017/11/20/armie-hammer-...</td>\n      <td>Armie Hammer calls out double standard between...</td>\n      <td>932740353605505025\\t932742389332152320\\t932742...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17399</th>\n      <td>gossipcop-1770549032</td>\n      <td>www.inquisitr.com/3101499/justin-bieber-blames...</td>\n      <td>Justin Bieber Blames Taylor Swift For Selena G...</td>\n      <td>732205554513379328\\t732224787351621632\\t732225...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T15:34:37.148201783Z",
     "start_time": "2023-04-30T15:34:37.123464619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2129 710 710\n"
     ]
    }
   ],
   "source": [
    "gossicop_fake['labels']= 0\n",
    "gossicop_real['labels']= 1\n",
    "\n",
    "politifact_fake['labels']= 0\n",
    "politifact_real['labels']= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T15:34:37.262892070Z",
     "start_time": "2023-04-30T15:34:37.134077605Z"
    }
   },
   "source": [
    "#### Concatenating the dataset (real and fake) and shuffling the dataset"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-29T13:33:48.300122Z",
     "end_time": "2023-04-29T13:33:48.332804Z"
    }
   },
   "outputs": [],
   "source": [
    "df_gossip = pd.concat([gossicop_real, gossicop_fake], ignore_index=True)\n",
    "df_gossip = shuffle(df_gossip)\n",
    "\n",
    "df_politifact = pd.concat([politifact_real, politifact_fake], ignore_index=True)\n",
    "df_politifact = shuffle(df_politifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T15:34:40.670146704Z",
     "start_time": "2023-04-30T15:34:37.181441056Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "df_gossip.columns, df_politifact.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-24T23:56:27.279699Z",
     "end_time": "2023-04-24T23:56:27.428021Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gossip.title.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T15:34:40.671409152Z",
     "start_time": "2023-04-30T15:34:40.657298802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU is allocated.\n"
     ]
    }
   ],
   "source": [
    "df_politifact.title.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-24T23:56:27.325545Z",
     "end_time": "2023-04-24T23:56:27.430159Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                        id                                           news_url  \\\n2773      gossipcop-927596  https://www.miami.com/miami-news/luis-fonsi-ca...   \n8510      gossipcop-884272  https://www.etonline.com/sofia-richie-and-scot...   \n6256      gossipcop-866058  https://www.hollywoodreporter.com/live-feed/ba...   \n7009      gossipcop-873690  https://people.com/home/tarek-el-moussa-on-pro...   \n17871  gossipcop-109358738  www.dailymail.co.uk/tvshowbiz/article-4862870/...   \n\n                                                   title  \\\n2773   Luis Fonsi can’t feel bad about turning 40. Hi...   \n8510   Sofia Richie and Scott Disick Share a Kiss on ...   \n6256   JoJo Breaks Down Her Final Two Men Ahead of 'T...   \n7009   Tarek and Christina El Moussa Produce New HGTV...   \n17871  Katie Holmes glows after confirming Jamie Foxx...   \n\n                                               tweet_ids  labels  \n2773   985738906435022848\\t985739320030973952\\t985740...       1  \n8510                                                 NaN       1  \n6256   884595148868526081\\t884595387692187648\\t884595...       1  \n7009   897566644884590592\\t897566802783477760\\t897566...       1  \n17871  905845659198205953\\t905875949257940995\\t905884...       0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>news_url</th>\n      <th>title</th>\n      <th>tweet_ids</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2773</th>\n      <td>gossipcop-927596</td>\n      <td>https://www.miami.com/miami-news/luis-fonsi-ca...</td>\n      <td>Luis Fonsi can’t feel bad about turning 40. Hi...</td>\n      <td>985738906435022848\\t985739320030973952\\t985740...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8510</th>\n      <td>gossipcop-884272</td>\n      <td>https://www.etonline.com/sofia-richie-and-scot...</td>\n      <td>Sofia Richie and Scott Disick Share a Kiss on ...</td>\n      <td>NaN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6256</th>\n      <td>gossipcop-866058</td>\n      <td>https://www.hollywoodreporter.com/live-feed/ba...</td>\n      <td>JoJo Breaks Down Her Final Two Men Ahead of 'T...</td>\n      <td>884595148868526081\\t884595387692187648\\t884595...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7009</th>\n      <td>gossipcop-873690</td>\n      <td>https://people.com/home/tarek-el-moussa-on-pro...</td>\n      <td>Tarek and Christina El Moussa Produce New HGTV...</td>\n      <td>897566644884590592\\t897566802783477760\\t897566...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>17871</th>\n      <td>gossipcop-109358738</td>\n      <td>www.dailymail.co.uk/tvshowbiz/article-4862870/...</td>\n      <td>Katie Holmes glows after confirming Jamie Foxx...</td>\n      <td>905845659198205953\\t905875949257940995\\t905884...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gossip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T15:34:40.764497023Z",
     "start_time": "2023-04-30T15:34:40.666812699Z"
    }
   },
   "outputs": [],
   "source": [
    "df_politifact.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-24T23:56:27.369695Z",
     "end_time": "2023-04-24T23:56:27.814542Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the class distribution\n",
    "class_distribution = df_gossip['labels'].value_counts()\n",
    "\n",
    "# Plot the class distribution\n",
    "class_distribution.plot(kind='bar')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Gossicop Class Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T15:34:40.785053987Z",
     "start_time": "2023-04-30T15:34:40.713437021Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the class distribution\n",
    "class_distribution = df_politifact['labels'].value_counts()\n",
    "\n",
    "# Plot the class distribution\n",
    "class_distribution.plot(kind='bar')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Politificat Class Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T15:34:40.785316729Z",
     "start_time": "2023-04-30T15:34:40.713694595Z"
    }
   },
   "outputs": [],
   "source": [
    "len(df_gossip), len(df_politifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T15:34:40.785479459Z",
     "start_time": "2023-04-30T15:34:40.713956709Z"
    }
   },
   "source": [
    "### To Choose the dataset to use (Images and CSV)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T15:34:40.785637300Z",
     "start_time": "2023-04-30T15:34:40.714202480Z"
    }
   },
   "source": [
    "### 1. To Run on the politifact Dataset, run the following cell"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T15:34:40.786270759Z",
     "start_time": "2023-04-30T15:34:40.714436936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 246 µs, sys: 131 µs, total: 377 µs\n",
      "Wall time: 364 µs\n"
     ]
    }
   ],
   "source": [
    "df = df_politifact.copy()\n",
    "image_path = 'politifact_images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. To Run on the gossip dataset, run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T15:49:31.297676667Z",
     "start_time": "2023-04-30T15:49:31.227175902Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df_gossip.copy()\n",
    "image_path = 'gossicop_images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T09:33:45.697927466Z",
     "start_time": "2023-04-30T09:33:45.695478103Z"
    }
   },
   "source": [
    "## Split the data into Train, Test and Validation"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-04-30T15:49:32.488103329Z",
     "start_time": "2023-04-30T15:49:32.438776504Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "import read_data\n",
    "df = read_data.load_dataset_gossipcop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-04-30T09:33:47.705356935Z",
     "start_time": "2023-04-30T09:33:47.649800120Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-04-30T15:49:33.203243878Z",
     "start_time": "2023-04-30T15:49:33.156791504Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-04-30T15:49:33.594538576Z",
     "start_time": "2023-04-30T15:49:33.579748533Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T09:33:49.292810740Z",
     "start_time": "2023-04-30T09:33:49.234376283Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into train (60%), val (20%), and test (20%) sets\n",
    "df_train, temp_df = train_test_split(df, test_size=0.4, random_state=42)\n",
    "df_val, df_test = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Check the length of each set\n",
    "print(len(df_train), len(df_val), len(df_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T15:49:34.325630277Z",
     "start_time": "2023-04-30T15:49:34.291874041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2649, 0.7351])\n"
     ]
    }
   ],
   "source": [
    "# # Split the data into train (70%), val (15%), and test (15%) sets\n",
    "# df_train, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "# df_val, df_test = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "#\n",
    "# # Check the length of each set\n",
    "# print(len(df_train), len(df_val), len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T15:49:41.853115998Z",
     "start_time": "2023-04-30T15:49:34.667752989Z"
    }
   },
   "source": [
    "### Loading Pretrained Transformer Models and Resnet"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "MultiModalModel(\n  (resnet): ResNet(\n    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (layer1): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer2): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer3): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (4): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (5): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer4): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n  )\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (text_linear): Linear(in_features=768, out_features=768, bias=True)\n  (image_linear): Linear(in_features=2048, out_features=768, bias=True)\n  (attention): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=3072, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=2, bias=True)\n  )\n)"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T15:49:41.926328903Z",
     "start_time": "2023-04-30T15:49:41.853502009Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load pre-trained models\n",
    "resnet = models.resnet18(pretrained=True) # resnet18, resnet34\n",
    "resnet = nn.Sequential(*list(resnet.children())[:-1])  # Remove the last layer to get embeddings\n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T15:49:41.947034419Z",
     "start_time": "2023-04-30T15:49:41.893514474Z"
    }
   },
   "source": [
    "### Assigning device to train the model"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": "                         id  \\\n14395      gossipcop-867129   \n3891       gossipcop-904181   \n5019       gossipcop-866433   \n2450       gossipcop-912279   \n19739  gossipcop-4882548283   \n\n                                                news_url  \\\n14395  https://people.com/parents/thomas-rhett-lauren...   \n3891   http://wstale.com/celebrities/lauren-conrad-st...   \n5019   https://people.com/tv/inside-bethenny-frankel-...   \n2450   https://people.com/music/travis-scott-pleads-g...   \n19739  www.express.co.uk/entertainment/films/989438/M...   \n\n                                                   title  \\\n14395  Southern Charm! See the Gorgeous Photos from T...   \n3891   Lauren Conrad Stirs The Hills Nostalgia With N...   \n5019   Inside Bethenny Frankel's 'Very Independent' R...   \n2450   Travis Scott Pleads Guilty to Disorderly Condu...   \n19739  Mamma Mia 2: Amanda Seyfried opens up on worki...   \n\n                                               tweet_ids  labels  \n14395  886256340016009216\\t886256749241651200\\t886256...       0  \n3891   949671170152026112\\t949671397600591872\\t949671...       0  \n5019   885175211309531140\\t885175286031056897\\t885175...       0  \n2450   961304414806990848\\t961304570839347200\\t961305...       0  \n19739                                 875691992960946177       1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>news_url</th>\n      <th>title</th>\n      <th>tweet_ids</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>14395</th>\n      <td>gossipcop-867129</td>\n      <td>https://people.com/parents/thomas-rhett-lauren...</td>\n      <td>Southern Charm! See the Gorgeous Photos from T...</td>\n      <td>886256340016009216\\t886256749241651200\\t886256...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3891</th>\n      <td>gossipcop-904181</td>\n      <td>http://wstale.com/celebrities/lauren-conrad-st...</td>\n      <td>Lauren Conrad Stirs The Hills Nostalgia With N...</td>\n      <td>949671170152026112\\t949671397600591872\\t949671...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5019</th>\n      <td>gossipcop-866433</td>\n      <td>https://people.com/tv/inside-bethenny-frankel-...</td>\n      <td>Inside Bethenny Frankel's 'Very Independent' R...</td>\n      <td>885175211309531140\\t885175286031056897\\t885175...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2450</th>\n      <td>gossipcop-912279</td>\n      <td>https://people.com/music/travis-scott-pleads-g...</td>\n      <td>Travis Scott Pleads Guilty to Disorderly Condu...</td>\n      <td>961304414806990848\\t961304570839347200\\t961305...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19739</th>\n      <td>gossipcop-4882548283</td>\n      <td>www.express.co.uk/entertainment/films/989438/M...</td>\n      <td>Mamma Mia 2: Amanda Seyfried opens up on worki...</td>\n      <td>875691992960946177</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T09:34:18.347433893Z",
     "start_time": "2023-04-30T09:34:18.293820833Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                        id                                           news_url  \\\n12233     gossipcop-876674  http://liverampup.com/entertainment/william-le...   \n13258     gossipcop-865215  http://toofab.com/2017/07/06/spice-girls-wanna...   \n20302  gossipcop-667682841  ew.com/tv/2017/07/13/billy-bush-not-hosting-fo...   \n13037     gossipcop-844552  https://www.fame10.com/entertainment/real-hous...   \n13988     gossipcop-899206  https://www.dailymail.co.uk/tvshowbiz/article-...   \n\n                                                   title  \\\n12233  William Levy Allegedly Cheated On Wife-Like Gi...   \n13258  Nicole Kidman, James Franco, Milo Ventimiglia ...   \n20302         No, Billy Bush isn't hosting a show on Fox   \n13037  Real Housewives: Ranking The 12 Shadiest Signi...   \n13988  Chloë Moretz 'wanted to hide' after Brooklyn B...   \n\n                                               tweet_ids  labels  \n12233  902708815526846464\\t902708829611327488\\t902708...       0  \n13258  882988553743085572\\t882988925802999808\\t882988...       0  \n20302  3085644868\\t3206664939\\t3646127827\\t1014232385...       1  \n13037  854742061550936064\\t854742374433529856\\t854742...       0  \n13988  939565900398768129\\t939566081613504512\\t939566...       0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>news_url</th>\n      <th>title</th>\n      <th>tweet_ids</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>12233</th>\n      <td>gossipcop-876674</td>\n      <td>http://liverampup.com/entertainment/william-le...</td>\n      <td>William Levy Allegedly Cheated On Wife-Like Gi...</td>\n      <td>902708815526846464\\t902708829611327488\\t902708...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13258</th>\n      <td>gossipcop-865215</td>\n      <td>http://toofab.com/2017/07/06/spice-girls-wanna...</td>\n      <td>Nicole Kidman, James Franco, Milo Ventimiglia ...</td>\n      <td>882988553743085572\\t882988925802999808\\t882988...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>20302</th>\n      <td>gossipcop-667682841</td>\n      <td>ew.com/tv/2017/07/13/billy-bush-not-hosting-fo...</td>\n      <td>No, Billy Bush isn't hosting a show on Fox</td>\n      <td>3085644868\\t3206664939\\t3646127827\\t1014232385...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13037</th>\n      <td>gossipcop-844552</td>\n      <td>https://www.fame10.com/entertainment/real-hous...</td>\n      <td>Real Housewives: Ranking The 12 Shadiest Signi...</td>\n      <td>854742061550936064\\t854742374433529856\\t854742...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13988</th>\n      <td>gossipcop-899206</td>\n      <td>https://www.dailymail.co.uk/tvshowbiz/article-...</td>\n      <td>Chloë Moretz 'wanted to hide' after Brooklyn B...</td>\n      <td>939565900398768129\\t939566081613504512\\t939566...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cpu\"#torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if str(device) == \"cpu\":\n",
    "    print(\"CPU is allocated.\")\n",
    "else:\n",
    "    print(\"GPU is allocated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T15:50:27.165966024Z",
     "start_time": "2023-04-30T15:49:43.201299213Z"
    }
   },
   "source": [
    "### Dataloader\n",
    "- For each of the title in the csv, when the image exist, read it and apply transformations (like converting to tensors)\n",
    "- When the post does not have an associated image, use a black image\n",
    "- Dynamically store the images depending on how many images are in the folder and stack them. The dataloader will work with any arbitrary number of images and thus no need to modify the loading when more images are incorporated"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n",
      "Training model in progress..\n",
      "---------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2857349cc20643ac9acbb3a6f9a2a894"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 3, 3, 224, 224]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[61], line 9\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mEPOCHS\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m10\u001B[39m)\n\u001B[0;32m----> 9\u001B[0m train_acc, train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43mloss_function\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdf_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrain loss \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_loss\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | Accuracy \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_acc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28mprint\u001B[39m()\n",
      "Cell \u001B[0;32mIn[55], line 13\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, data_loader, criterion, optimizer, device, num_examples)\u001B[0m\n\u001B[1;32m     10\u001B[0m labels \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimages\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m _, preds \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmax(outputs, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     15\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m criterion(outputs\u001B[38;5;241m.\u001B[39msqueeze(), labels\u001B[38;5;241m.\u001B[39mfloat())\n",
      "File \u001B[0;32m~/elvin/UT Courses/THESIS/SOURCE CODES/MIMCAN/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[53], line 22\u001B[0m, in \u001B[0;36mMultiModalModel.forward\u001B[0;34m(self, text_inputs, image_inputs)\u001B[0m\n\u001B[1;32m     19\u001B[0m text_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtext_linear(text_outputs)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Image feature extraction\u001B[39;00m\n\u001B[0;32m---> 22\u001B[0m img_embeddings \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresnet(torch\u001B[38;5;241m.\u001B[39munsqueeze(img, \u001B[38;5;241m0\u001B[39m)) \u001B[38;5;28;01mfor\u001B[39;00m img \u001B[38;5;129;01min\u001B[39;00m image_inputs]\n\u001B[1;32m     23\u001B[0m img_embeddings \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack(img_embeddings)\n\u001B[1;32m     24\u001B[0m image_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimage_linear(img_embeddings)\n",
      "Cell \u001B[0;32mIn[53], line 22\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     19\u001B[0m text_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtext_linear(text_outputs)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Image feature extraction\u001B[39;00m\n\u001B[0;32m---> 22\u001B[0m img_embeddings \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m img \u001B[38;5;129;01min\u001B[39;00m image_inputs]\n\u001B[1;32m     23\u001B[0m img_embeddings \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack(img_embeddings)\n\u001B[1;32m     24\u001B[0m image_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimage_linear(img_embeddings)\n",
      "File \u001B[0;32m~/elvin/UT Courses/THESIS/SOURCE CODES/MIMCAN/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/elvin/UT Courses/THESIS/SOURCE CODES/MIMCAN/env/lib/python3.10/site-packages/torchvision/models/resnet.py:285\u001B[0m, in \u001B[0;36mResNet.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    284\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 285\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_forward_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/elvin/UT Courses/THESIS/SOURCE CODES/MIMCAN/env/lib/python3.10/site-packages/torchvision/models/resnet.py:268\u001B[0m, in \u001B[0;36mResNet._forward_impl\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_forward_impl\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m    267\u001B[0m     \u001B[38;5;66;03m# See note [TorchScript super()]\u001B[39;00m\n\u001B[0;32m--> 268\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    269\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn1(x)\n\u001B[1;32m    270\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu(x)\n",
      "File \u001B[0;32m~/elvin/UT Courses/THESIS/SOURCE CODES/MIMCAN/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/elvin/UT Courses/THESIS/SOURCE CODES/MIMCAN/env/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    462\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 463\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/elvin/UT Courses/THESIS/SOURCE CODES/MIMCAN/env/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[1;32m    457\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[1;32m    458\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[0;32m--> 459\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    460\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 3, 3, 224, 224]"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "import pdb\n",
    "class MultiModalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, image_folder, tokenizer, transform):\n",
    "        self.df = df\n",
    "        self.image_folder = image_folder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        # pdb.set_trace()\n",
    "        text, post_id, labels = row[\"title\"], row[\"id\"], row[\"labels\"]\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "        IMG_SIZE = 3\n",
    "        images_path = os.path.join(self.image_folder, post_id)\n",
    "        images = []\n",
    "        if os.path.exists(images_path):\n",
    "            image_files = os.listdir(images_path)\n",
    "\n",
    "            for img in image_files:\n",
    "                if len(images) >= IMG_SIZE:\n",
    "                    break\n",
    "                try:\n",
    "                    image = Image.open(os.path.join(images_path, img)).convert('RGB')\n",
    "                    if image.mode != 'RGB':\n",
    "                        image = image.convert('RGB')\n",
    "                    images.append(image)\n",
    "                # Handling Exception and randomly initializing pixels\n",
    "                except Exception:\n",
    "                    images.append(Image.new(\"RGB\", (224, 224), \"black\"))\n",
    "\n",
    "        while len(images) < IMG_SIZE:\n",
    "            images.append(Image.new(\"RGB\", (224, 224), \"black\"))\n",
    "            \n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "        images = [self.transform(img) for img in images]\n",
    "        images_tensor = torch.stack(images)\n",
    "        sample = {'text': inputs, 'images': images_tensor, 'labels': labels}\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T09:28:15.422269579Z",
     "start_time": "2023-04-30T09:28:15.419663210Z"
    }
   },
   "source": [
    "### Data Augmentation"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": "2607"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T08:43:40.766428259Z",
     "start_time": "2023-04-30T08:43:40.728114457Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transform function for image processing (training)\n",
    "# Performing data augmentation by random resizing, cropping\n",
    "# and flipping images in order to artificially create new\n",
    "# image data per training epoch\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.255]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# # Define image transformations\n",
    "# train_transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "# ])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-04-30T08:43:40.766528263Z",
     "start_time": "2023-04-30T08:43:40.728281992Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T08:43:40.766627359Z",
     "start_time": "2023-04-30T08:43:40.728396382Z"
    }
   },
   "outputs": [],
   "source": [
    "# Just normalization for validation\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.255]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# val_transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "# ])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-04-30T08:43:40.766730366Z",
     "start_time": "2023-04-30T08:43:40.728543245Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T08:43:40.766960752Z",
     "start_time": "2023-04-30T08:43:40.728629422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 174 µs, sys: 115 µs, total: 289 µs\n",
      "Wall time: 288 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create the dataset\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "\n",
    "train_dataset = MultiModalDataset(df_train, image_path, tokenizer, train_transform )\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True, prefetch_factor=2)\n",
    "\n",
    "val_dataset = MultiModalDataset(df_val, image_path, tokenizer, train_transform )\n",
    "validate_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True, prefetch_factor=2)\n",
    "\n",
    "test_dataset = MultiModalDataset(df_test, image_path, tokenizer, val_transform )\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True, prefetch_factor=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation\n",
    "- The model has the following major components\n",
    "    - Pretrained BERT model for getting embeddings for the title and a pretrained resnet for the image embeddings \n",
    "    - Performs transfer learning based on the pretrained models (BERT and RESNET)\n",
    "    - Tokenization of the title input using transformers\n",
    "    - Cross attention mechanism implemented with 4 attention heads (Increased the number of heads for improvement in performance)\n",
    "        - Each image attents to a title i.e if there are 5 images in the article, they will attent to the same title or text given by id \n",
    "        - For each of the image or if it exists for a certain post, feed in the images and compute cross attention for the image to title attention (each image attenting to the post)\n",
    "        - The attention query in this case is the text (title), key is the image embeddings and value is the image embeddings\n",
    "        - It is flexible enough to take in any number of images per post\n",
    "        - The cross attention output is then averaged \n",
    "     - Additional functionalities like adding batch normalization to the image model layers, ReLU activation function for non linearity and dropout \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import pdb"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-04-30T08:43:41.002678565Z",
     "start_time": "2023-04-30T08:43:40.937605190Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T09:01:48.722593323Z",
     "start_time": "2023-04-30T09:01:48.607846718Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, bert, resnet):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.resnet = resnet\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        \n",
    "        # Defining the attention mechanism for the model\n",
    "        self.image_to_title_attention = nn.MultiheadAttention(bert.config.hidden_size, num_heads=4)  # Increase num_heads\n",
    "        \n",
    "        self.linear = nn.Linear(2048, bert.config.hidden_size)\n",
    "        self.norm = nn.BatchNorm1d(bert.config.hidden_size)\n",
    "        self.relu = nn.ReLU()  # Add ReLU activation\n",
    "        self.hidden = nn.Linear(bert.config.hidden_size, bert.config.hidden_size)  # Add hidden layer\n",
    "        self.classifier = nn.Linear(bert.config.hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs, images):\n",
    "        # Process text input\n",
    "        # pdb.set_trace()\n",
    "        text_output = self.bert(**inputs).last_hidden_state[:, 0, :]\n",
    "\n",
    "        # Process image input\n",
    "        # print(torch.unsqueeze(images[0], 0).view(-1).shape)\n",
    "        img_embeddings = [self.resnet(img.unsqueeze(0)).view(-1) for img in images]\n",
    "        img_embeddings = torch.stack(img_embeddings)\n",
    "        img_embeddings = self.linear(img_embeddings)\n",
    "        img_embeddings = self.norm(img_embeddings)  # Apply batch normalization\n",
    "        img_embeddings = self.relu(img_embeddings)  # Apply ReLU activation\n",
    "\n",
    "        # Calculate attention between text and each image\n",
    "        attention_outputs = []\n",
    "        for img_emb in img_embeddings:\n",
    "            img_emb = img_emb.view(1, 1, 768)\n",
    "            # text_output.unsqueeze(1).shape (1, batch_size, hidden_size) => (1, 2, 768)\n",
    "            #img_emb.shape => (1, 1, hidden_size)\n",
    "            att_out, _ = self.image_to_title_attention(text_output.unsqueeze(1), img_emb, img_emb)\n",
    "            attention_outputs.append(att_out)\n",
    "\n",
    "        # Average attention outputs\n",
    "        attention_output = torch.stack(attention_outputs).mean(dim=0)\n",
    "\n",
    "        # Classifier\n",
    "        logits = self.hidden(attention_output.squeeze(1))  # Apply hidden layer\n",
    "        logits = self.drop(logits)  # Apply dropout to the hidden layer\n",
    "        logits = self.classifier(logits)\n",
    "        return self.sigmoid(logits)\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training Configuration\n",
    "- Computing the class weights for the dataset so that we put different weights on the classes. It will help in class balancing during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T09:01:49.460401938Z",
     "start_time": "2023-04-30T09:01:49.397776156Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_class_weights(dataframe):\n",
    "    \n",
    "    # Count labels per class / subtype of Fake News in training set split\n",
    "    # in sorted order 0, 1 and put into label_count list\n",
    "    label_count = [dataframe[\"labels\"].value_counts().sort_index(0)[0],\n",
    "                   dataframe[\"labels\"].value_counts().sort_index(0)[1]]\n",
    "\n",
    "    # Calculate weights per class by subtracting from 1 label_count per class divided\n",
    "    # by sum of all label_counts\n",
    "    class_weights = [1 - (x / sum(label_count)) for x in label_count]\n",
    "\n",
    "    class_weights = torch.FloatTensor(class_weights)\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Routine\n",
    "- Training routing for the model. Some of the additional techniques applied include clipping the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T09:01:49.740966073Z",
     "start_time": "2023-04-30T09:01:49.711625174Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, data_loader, criterion, optimizer, device, num_examples):\n",
    "    print(\"Training model in progress..\")\n",
    "    print(\"-\" * 15)\n",
    "    model = model.train()\n",
    "    train_losses = []\n",
    "    correct_preds = 0\n",
    "    for data in tqdm(data_loader):\n",
    "        inputs = {k: v.squeeze(1).to(device) for k, v in data['text'].items()}\n",
    "        images = data['images'].to(device)\n",
    "        labels = data['labels'].to(device)\n",
    "    \n",
    "        # Forward pass\n",
    "        outputs = model(inputs, images.squeeze(1))\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        train_loss = criterion(outputs.squeeze(), labels.float())\n",
    "        \n",
    "        correct_preds += torch.sum(preds == labels)\n",
    "        train_losses.append(train_loss.item())\n",
    "        train_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "    # Return train_acc and train_loss values\n",
    "    return correct_preds.double() / num_examples, np.mean(train_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T09:01:49.941907238Z",
     "start_time": "2023-04-30T09:01:49.873036612Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, loss_function, device, num_examples):\n",
    "    print(\"validation of the model in progress...\")\n",
    "    print(\"-\" * 15)\n",
    "    model = model.eval()\n",
    "    val_losses = []\n",
    "    correct_preds = 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(data_loader):\n",
    "            # Move data to the device\n",
    "            inputs = {k: v.squeeze(1).to(device) for k, v in data['text'].items()}\n",
    "            images = data['images'].to(device)\n",
    "            labels = data['labels'].to(device)\n",
    "            \n",
    "            outputs = model(inputs, images.squeeze(1))\n",
    "            \n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            \n",
    "            val_loss = loss_function(outputs.squeeze(), labels.float())\n",
    "            correct_preds += torch.sum(preds == labels)\n",
    "            val_losses.append(val_loss.item())\n",
    "    return correct_preds.double() / num_examples, np.mean(val_losses)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T09:01:50.169784312Z",
     "start_time": "2023-04-30T09:01:50.144973891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2635, 0.7365])\n"
     ]
    }
   ],
   "source": [
    "# Calculate class weights on basis of training split dataframe and print weight tensor\n",
    "class_weights = get_class_weights(df_train)\n",
    "\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T09:01:50.411456777Z",
     "start_time": "2023-04-30T09:01:50.306043374Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "MultiModalModel(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (resnet): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n  )\n  (drop): Dropout(p=0.3, inplace=False)\n  (image_to_title_attention): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n  )\n  (linear): Linear(in_features=2048, out_features=768, bias=True)\n  (norm): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU()\n  (hidden): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=1, bias=True)\n  (sigmoid): Sigmoid()\n)"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MultiModalModel(bert, resnet)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T09:01:51.285460977Z",
     "start_time": "2023-04-30T09:01:51.221359485Z"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "# Initializing weighted Cross Entropy Loss function and assignment to device\n",
    "loss_function = nn.CrossEntropyLoss(weight=class_weights).to(device)\n",
    "# Set up the loss function and optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "                         id  \\\n14686      gossipcop-905277   \n17412  gossipcop-6828006698   \n1326       gossipcop-865378   \n21761  gossipcop-9915206962   \n2412       gossipcop-850516   \n\n                                                news_url  \\\n14686  https://www.channel24.co.za/Gossip/News/watch-...   \n17412  people.com/style/justin-bieber-hailey-baldwin-...   \n1326   https://people.com/tv/inside-julianne-hough-br...   \n21761  www.inquisitr.com/4790273/blake-shelton-jokes-...   \n2412   https://www.thewrap.com/once-upon-time-season-...   \n\n                                                   title  \\\n14686  WATCH: David Beckham launches sexy ad campaign...   \n17412  Are Justin Bieber and Hailey Baldwin Already M...   \n1326   Dancing Under the Stars: Julianne Hough's Perf...   \n21761  Blake Shelton Jokes Gwen Stefani ‘Gave Birth T...   \n2412   ‘Once Upon a Time': Creators Defend Cast Shake...   \n\n                                               tweet_ids  labels  \n14686  950901801271922689\\t950902108160577536\\t950902...       0  \n17412  74339314045157376\\t237856098332340224\\t5924446...       1  \n1326   883266867535773696\\t883267037358727168\\t883267...       0  \n21761  964814953143291904\\t964819241932386304\\t964819...       1  \n2412                                                 NaN       0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>news_url</th>\n      <th>title</th>\n      <th>tweet_ids</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>14686</th>\n      <td>gossipcop-905277</td>\n      <td>https://www.channel24.co.za/Gossip/News/watch-...</td>\n      <td>WATCH: David Beckham launches sexy ad campaign...</td>\n      <td>950901801271922689\\t950902108160577536\\t950902...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17412</th>\n      <td>gossipcop-6828006698</td>\n      <td>people.com/style/justin-bieber-hailey-baldwin-...</td>\n      <td>Are Justin Bieber and Hailey Baldwin Already M...</td>\n      <td>74339314045157376\\t237856098332340224\\t5924446...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1326</th>\n      <td>gossipcop-865378</td>\n      <td>https://people.com/tv/inside-julianne-hough-br...</td>\n      <td>Dancing Under the Stars: Julianne Hough's Perf...</td>\n      <td>883266867535773696\\t883267037358727168\\t883267...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21761</th>\n      <td>gossipcop-9915206962</td>\n      <td>www.inquisitr.com/4790273/blake-shelton-jokes-...</td>\n      <td>Blake Shelton Jokes Gwen Stefani ‘Gave Birth T...</td>\n      <td>964814953143291904\\t964819241932386304\\t964819...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2412</th>\n      <td>gossipcop-850516</td>\n      <td>https://www.thewrap.com/once-upon-time-season-...</td>\n      <td>‘Once Upon a Time': Creators Defend Cast Shake...</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-04-30T09:01:51.588950222Z",
     "start_time": "2023-04-30T09:01:51.555371285Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T09:01:53.599644779Z",
     "start_time": "2023-04-30T09:01:52.637424543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n",
      "Training model in progress..\n",
      "---------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a6f1a95abd5f466d9aeaeda8424e9c9e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 5.80 GiB total capacity; 5.03 GiB already allocated; 2.94 MiB free; 5.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[39], line 9\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mEPOCHS\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m10\u001B[39m)\n\u001B[0;32m----> 9\u001B[0m train_acc, train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43mloss_function\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdf_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrain loss \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_loss\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | Accuracy \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_acc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28mprint\u001B[39m()\n",
      "Cell \u001B[0;32mIn[33], line 13\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, data_loader, criterion, optimizer, device, num_examples)\u001B[0m\n\u001B[1;32m     10\u001B[0m labels \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimages\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m _, preds \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmax(outputs, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     15\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m criterion(outputs\u001B[38;5;241m.\u001B[39msqueeze(), labels\u001B[38;5;241m.\u001B[39mfloat())\n",
      "File \u001B[0;32m~/elvin/UT Courses/THESIS/SOURCE CODES/MIMCAN/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[31], line 21\u001B[0m, in \u001B[0;36mMultiModalModel.forward\u001B[0;34m(self, inputs, images)\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, images):\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;66;03m# Process text input\u001B[39;00m\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;66;03m# pdb.set_trace()\u001B[39;00m\n\u001B[0;32m---> 21\u001B[0m     text_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbert\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mlast_hidden_state[:, \u001B[38;5;241m0\u001B[39m, :]\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;66;03m# Process image input\u001B[39;00m\n\u001B[1;32m     24\u001B[0m     img_embeddings \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/elvin/UT Courses/THESIS/SOURCE CODES/MIMCAN/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/elvin/UT Courses/THESIS/SOURCE CODES/MIMCAN/env/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1020\u001B[0m, in \u001B[0;36mBertModel.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1011\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[1;32m   1013\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(\n\u001B[1;32m   1014\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m   1015\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1018\u001B[0m     past_key_values_length\u001B[38;5;241m=\u001B[39mpast_key_values_length,\n\u001B[1;32m   1019\u001B[0m )\n\u001B[0;32m-> 1020\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1021\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1022\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1023\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1024\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1025\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1026\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1027\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1028\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1029\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1030\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1031\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1032\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1033\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/elvin/UT Courses/THESIS/SOURCE CODES/MIMCAN/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/elvin/UT Courses/THESIS/SOURCE CODES/MIMCAN/env/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:610\u001B[0m, in \u001B[0;36mBertEncoder.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    601\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mcheckpoint\u001B[38;5;241m.\u001B[39mcheckpoint(\n\u001B[1;32m    602\u001B[0m         create_custom_forward(layer_module),\n\u001B[1;32m    603\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    607\u001B[0m         encoder_attention_mask,\n\u001B[1;32m    608\u001B[0m     )\n\u001B[1;32m    609\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 610\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    611\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    612\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    613\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    614\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    615\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    616\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    617\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    618\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    620\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    621\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[0;32m~/elvin/UT Courses/THESIS/SOURCE CODES/MIMCAN/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/elvin/UT Courses/THESIS/SOURCE CODES/MIMCAN/env/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:495\u001B[0m, in \u001B[0;36mBertLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    483\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    484\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    485\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    492\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[1;32m    493\u001B[0m     \u001B[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001B[39;00m\n\u001B[1;32m    494\u001B[0m     self_attn_past_key_value \u001B[38;5;241m=\u001B[39m past_key_value[:\u001B[38;5;241m2\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 495\u001B[0m     self_attention_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    496\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    497\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    499\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mself_attn_past_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    501\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    502\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m self_attention_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    504\u001B[0m     \u001B[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001B[39;00m\n",
      "File \u001B[0;32m~/elvin/UT Courses/THESIS/SOURCE CODES/MIMCAN/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/elvin/UT Courses/THESIS/SOURCE CODES/MIMCAN/env/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:425\u001B[0m, in \u001B[0;36mBertAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    415\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    416\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    417\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    423\u001B[0m     output_attentions: Optional[\u001B[38;5;28mbool\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    424\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[0;32m--> 425\u001B[0m     self_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    426\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    427\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    428\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    429\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    430\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    431\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    432\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    433\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    434\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput(self_outputs[\u001B[38;5;241m0\u001B[39m], hidden_states)\n\u001B[1;32m    435\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (attention_output,) \u001B[38;5;241m+\u001B[39m self_outputs[\u001B[38;5;241m1\u001B[39m:]  \u001B[38;5;66;03m# add attentions if we output them\u001B[39;00m\n",
      "File \u001B[0;32m~/elvin/UT Courses/THESIS/SOURCE CODES/MIMCAN/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/elvin/UT Courses/THESIS/SOURCE CODES/MIMCAN/env/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:323\u001B[0m, in \u001B[0;36mBertSelfAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    320\u001B[0m     past_key_value \u001B[38;5;241m=\u001B[39m (key_layer, value_layer)\n\u001B[1;32m    322\u001B[0m \u001B[38;5;66;03m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001B[39;00m\n\u001B[0;32m--> 323\u001B[0m attention_scores \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmatmul\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_layer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey_layer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtranspose\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mposition_embedding_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrelative_key\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mposition_embedding_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrelative_key_query\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    326\u001B[0m     query_length, key_length \u001B[38;5;241m=\u001B[39m query_layer\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m], key_layer\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m]\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 5.80 GiB total capacity; 5.03 GiB already allocated; 2.94 MiB free; 5.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "\n",
    "# Iteration times the total number of epochs\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    print(\"-\" * 10)\n",
    "\n",
    "    train_acc, train_loss = train_model(\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        loss_function,\n",
    "        optimizer,\n",
    "        device,\n",
    "        len(df_train)\n",
    "    )\n",
    "    \n",
    "\n",
    "    print(f\"Train loss {train_loss} | Accuracy {train_acc}\")\n",
    "    print()\n",
    "    val_acc, val_loss = evaluate_model(\n",
    "            model,\n",
    "            validate_dataloader,\n",
    "            loss_function,\n",
    "            device,\n",
    "            len(df_val)\n",
    "    )\n",
    "\n",
    "    print(f\"Val   loss {val_loss} | Accuracy {val_acc}\")\n",
    "    print()\n",
    "\n",
    "print()\n",
    "print(\"Completed Training!\")\n",
    "print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T08:38:12.940390915Z",
     "start_time": "2023-04-30T08:38:12.934746784Z"
    }
   },
   "source": [
    "### Plotting the output results"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": "2694"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-29T10:55:38.025739058Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting training and validation accuracy curves across the epochs\n",
    "plt.plot(train_acc, color=\"green\", label=\"Training Accuracy\")\n",
    "plt.plot(val_acc, color=\"red\", label=\"Validation Accuracy\")\n",
    "\n",
    "plt.title(\"Training History\")\n",
    "# Defining x- and y-axis labels\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss, color=\"blue\", label=\"Training Loss\")\n",
    "plt.plot(val_loss, color=\"orange\", label=\"Validation Loss\")\n",
    "plt.title(\"Training History\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, data_loader, loss_function, device, num_examples):\n",
    "    print(\"Testing model in progress...\")\n",
    "    print(\"-\" * 15)\n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    correct_preds = 0\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "    real_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(data_loader):\n",
    "            inputs = {k: v.squeeze(1).to(device) for k, v in data['text'].items()}\n",
    "            images = data['images'].to(device)\n",
    "            labels = data['labels'].to(device)\n",
    "            \n",
    "            outputs = model(inputs, images.squeeze(1))\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            test_loss = loss_function(outputs.squeeze(), labels.float())\n",
    "            correct_preds += torch.sum(preds == labels)\n",
    "            test_losses.append(test_loss.item())\n",
    "            predictions.extend(preds)\n",
    "            prediction_probs.extend(outputs)\n",
    "            real_labels.extend(labels)\n",
    "    test_acc = correct_preds.double() / num_examples\n",
    "    test_loss = np.mean(test_losses)\n",
    "    predictions = torch.stack(predictions)\n",
    "    prediction_probs = torch.stack(prediction_probs)\n",
    "    real_labels = torch.stack(real_labels)\n",
    "    \n",
    "    # Return test_acc, test_loss, predictions, prediction_probs, real_labels\n",
    "    return test_acc, test_loss, predictions, prediction_probs, real_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing model on test data split and initilaizing test values\n",
    "test_acc, test_loss, y_preds, y_prediction_probs, y_test = test_model(\n",
    "    model,\n",
    "    test_dataloader,\n",
    "    loss_function,\n",
    "    device,\n",
    "    len(df_test)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing model test accuracy\n",
    "print(f\"Model testing accuracy for classifier:  {test_acc*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score, roc_auc_score, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting classification report\n",
    "print(classification_report(y_test.cpu(), y_preds.cpu(), target_names=CLASS_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or individual evaluations of the results\n",
    "print(\"f1 score is: \", f1_score(y_test.cpu(), y_preds.cpu(), average='macro'))\n",
    "print(\"precision score is: \", precision_score(y_test.cpu(), y_preds.cpu(), average='macro'))\n",
    "print(\"recall score is: \", recall_score(y_test.cpu(), y_preds.cpu(), average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"auc score for the model: \", roc_auc_score(y_test.cpu(), 1 - y_prediction_probs.cpu()[:,1], multi_class=\"ovo\", average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(confusion_matrix):\n",
    "    hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Purples\")\n",
    "    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha=\"right\")\n",
    "    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha=\"right\")\n",
    "    # Set x- and y-axis labels\n",
    "    plt.ylabel(\"Fakeddit Dataset Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.title(\"Confusion Matrix for the model\")\n",
    "    plt.tight_layout\n",
    "    plt.show()\n",
    "\n",
    "# Initialize confusion_matrix with y_test (ground truth labels) and predicted labels\n",
    "cm = confusion_matrix(y_test.cpu(), y_preds.cpu())\n",
    "df_cm = pd.DataFrame(cm, index=CLASS_NAMES, columns=CLASS_NAMES)\n",
    "plot_confusion_matrix(df_cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting AUC and ROC score for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create ROC curve\n",
    "def plot_auc(y_test, y_pred_proba):\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test,  1 - y_pred_proba[:,1])\n",
    "    plt.plot(fpr,tpr)\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.title(\"ROC curve of the model\")\n",
    "    plt.tight_layout\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc(y_test.cpu(), y_prediction_probs.cpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
