{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Python Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision, transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageFile, UnidentifiedImageError\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, cohen_kappa_score\n",
    "from textwrap import wrap\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from transformers import AdamW, BertModel, BertTokenizer, DistilBertModel\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gossicop_fake = pd.read_csv('new_data/gossipcop_fake.csv')\n",
    "gossicop_real = pd.read_csv('new_data/gossipcop_real.csv')\n",
    "politifact_fake = pd.read_csv('new_data/politifact_fake.csv')\n",
    "politifact_real = pd.read_csv('new_data/politifact_real.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding labels to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gossicop_fake['labels']= 0\n",
    "gossicop_real['labels']= 1\n",
    "\n",
    "politifact_fake['labels']= 0\n",
    "politifact_real['labels']= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating the dataset (real and fake) and shuffling the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gossip = pd.concat([gossicop_real, gossicop_fake], ignore_index=True)\n",
    "df_gossip = shuffle(df_gossip)\n",
    "\n",
    "df_politifact = pd.concat([politifact_real, politifact_fake], ignore_index=True)\n",
    "df_politifact = shuffle(df_politifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['id', 'news_url', 'title', 'tweet_ids', 'labels'], dtype='object'),\n",
       " Index(['id', 'news_url', 'title', 'tweet_ids', 'labels'], dtype='object'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gossip.columns, df_politifact.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gossip.title.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_politifact.title.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>news_url</th>\n",
       "      <th>title</th>\n",
       "      <th>tweet_ids</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17117</th>\n",
       "      <td>gossipcop-9755892564</td>\n",
       "      <td>www.etonline.com/media/videos/kim-kardashian-r...</td>\n",
       "      <td>Kim Kardashian Reveals Her Limit on Kids With ...</td>\n",
       "      <td>974158416703766528\\t1003401538864369664</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17258</th>\n",
       "      <td>gossipcop-29199720</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Miley Cyrus Liam Hemsworth Honeymoon</td>\n",
       "      <td>167704576621953024\\t173645082740793344\\t176788...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21857</th>\n",
       "      <td>gossipcop-3172607624</td>\n",
       "      <td>www.imdb.com/news/ni61578932</td>\n",
       "      <td>Keith Urban Upset About Nicole Kidman, Colin F...</td>\n",
       "      <td>915635939623493637\\t915642136888008704\\t915646...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5542</th>\n",
       "      <td>gossipcop-885290</td>\n",
       "      <td>https://www.nickiswift.com/89669/aaron-carter-...</td>\n",
       "      <td>Aaron Carter leaves rehab early</td>\n",
       "      <td>916304045433860097\\t916304110101680128\\t916305...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>gossipcop-844633</td>\n",
       "      <td>http://www.newslocker.com/en-us/news/celebrity...</td>\n",
       "      <td>Yikes! Kyle Is Furious After He Loses His Movi...</td>\n",
       "      <td>855786248132276225\\t855786718808612864\\t855786...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "17117  gossipcop-9755892564   \n",
       "17258    gossipcop-29199720   \n",
       "21857  gossipcop-3172607624   \n",
       "5542       gossipcop-885290   \n",
       "822        gossipcop-844633   \n",
       "\n",
       "                                                news_url  \\\n",
       "17117  www.etonline.com/media/videos/kim-kardashian-r...   \n",
       "17258                                                NaN   \n",
       "21857                       www.imdb.com/news/ni61578932   \n",
       "5542   https://www.nickiswift.com/89669/aaron-carter-...   \n",
       "822    http://www.newslocker.com/en-us/news/celebrity...   \n",
       "\n",
       "                                                   title  \\\n",
       "17117  Kim Kardashian Reveals Her Limit on Kids With ...   \n",
       "17258               Miley Cyrus Liam Hemsworth Honeymoon   \n",
       "21857  Keith Urban Upset About Nicole Kidman, Colin F...   \n",
       "5542                     Aaron Carter leaves rehab early   \n",
       "822    Yikes! Kyle Is Furious After He Loses His Movi...   \n",
       "\n",
       "                                               tweet_ids  labels  \n",
       "17117            974158416703766528\\t1003401538864369664       0  \n",
       "17258  167704576621953024\\t173645082740793344\\t176788...       0  \n",
       "21857  915635939623493637\\t915642136888008704\\t915646...       0  \n",
       "5542   916304045433860097\\t916304110101680128\\t916305...       1  \n",
       "822    855786248132276225\\t855786718808612864\\t855786...       1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gossip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>news_url</th>\n",
       "      <th>title</th>\n",
       "      <th>tweet_ids</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>politifact4433</td>\n",
       "      <td>http://www.fedupthebook.com/</td>\n",
       "      <td>Fed Up! Our Fight to Save America from Washing...</td>\n",
       "      <td>24006328852\\t24060358023\\t29526066043\\t2954558...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>politifact9438</td>\n",
       "      <td>https://web.archive.org/web/20130314224614/htt...</td>\n",
       "      <td>Everything you need to know about the drone de...</td>\n",
       "      <td>260532526676848641\\t310028760328073216\\t310029...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>politifact13751</td>\n",
       "      <td>https://web.archive.org/web/20170310140734/htt...</td>\n",
       "      <td>BREAKING: Trump Caught INVESTING in Dakota Pip...</td>\n",
       "      <td>829406690772660225\\t829424287081758722\\t829433...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>politifact15512</td>\n",
       "      <td>http://www.thecommonsenseshow.com/pope-calls-f...</td>\n",
       "      <td>Pope Calls for World Wide Gun Confiscation Exc...</td>\n",
       "      <td>992530484008706048\\t992530613969223680\\t992532...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>politifact14436</td>\n",
       "      <td>allnews4us.com/politics/whoopi-goldberg-humili...</td>\n",
       "      <td>Whoopi Goldberg Humiliated, Handcuffed And Dra...</td>\n",
       "      <td>876291648497627140\\t876342825834696705\\t876368...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                           news_url  \\\n",
       "528   politifact4433                       http://www.fedupthebook.com/   \n",
       "555   politifact9438  https://web.archive.org/web/20130314224614/htt...   \n",
       "935  politifact13751  https://web.archive.org/web/20170310140734/htt...   \n",
       "803  politifact15512  http://www.thecommonsenseshow.com/pope-calls-f...   \n",
       "880  politifact14436  allnews4us.com/politics/whoopi-goldberg-humili...   \n",
       "\n",
       "                                                 title  \\\n",
       "528  Fed Up! Our Fight to Save America from Washing...   \n",
       "555  Everything you need to know about the drone de...   \n",
       "935  BREAKING: Trump Caught INVESTING in Dakota Pip...   \n",
       "803  Pope Calls for World Wide Gun Confiscation Exc...   \n",
       "880  Whoopi Goldberg Humiliated, Handcuffed And Dra...   \n",
       "\n",
       "                                             tweet_ids  labels  \n",
       "528  24006328852\\t24060358023\\t29526066043\\t2954558...       1  \n",
       "555  260532526676848641\\t310028760328073216\\t310029...       1  \n",
       "935  829406690772660225\\t829424287081758722\\t829433...       0  \n",
       "803  992530484008706048\\t992530613969223680\\t992532...       0  \n",
       "880  876291648497627140\\t876342825834696705\\t876368...       0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_politifact.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the class distribution\n",
    "class_distribution = df_gossip['labels'].value_counts()\n",
    "\n",
    "# Plot the class distribution\n",
    "class_distribution.plot(kind='bar')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Gossicop Class Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the class distribution\n",
    "class_distribution = df_politifact['labels'].value_counts()\n",
    "\n",
    "# Plot the class distribution\n",
    "class_distribution.plot(kind='bar')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Politificat Class Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22140, 1056)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_gossip), len(df_politifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Choose the dataset to use (Images and CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. To Run on the politifact Dataset, run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_politifact.copy()\n",
    "image_path = 'images/politifact/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. To Run on the gossip dataset, run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_gossip.copy()\n",
    "image_path = 'images/gossicop/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into Train, Test and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13284 4428 4428\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train (60%), val (20%), and test (20%) sets\n",
    "df_train, temp_df = train_test_split(df, test_size=0.4, random_state=42)\n",
    "df_val, df_test = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Check the length of each set\n",
    "print(len(df_train), len(df_val), len(df_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15498 3321 3321\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train (70%), val (15%), and test (15%) sets\n",
    "df_train, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "df_val, df_test = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Check the length of each set\n",
    "print(len(df_train), len(df_val), len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pretrained Transformer Models and Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained models\n",
    "resnet = models.resnet18(pretrained=True) # resnet18, resnet34\n",
    "resnet = nn.Sequential(*list(resnet.children())[:-1])  # Remove the last layer to get embeddings\n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning device to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU is allocated.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if str(device) == \"cpu\":\n",
    "    print(\"CPU is allocated.\")\n",
    "else:\n",
    "    print(\"GPU is allocated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "- For each of the title in the csv, when the image exist, read it and apply transformations (like converting to tensors)\n",
    "- When the post does not have an associated image, use a black image\n",
    "- Dynamically store the images depending on how many images are in the folder and stack them. The dataloader will work with any arbitrary number of images and thus no need to modify the loading when more images are incorporated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class MultiModalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, image_folder, tokenizer, transform):\n",
    "        self.df = df\n",
    "        self.image_folder = image_folder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        #pdb.set_trace()\n",
    "        text, post_id, labels = row[\"title\"], row[\"id\"], row[\"labels\"]\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "        images_path = os.path.join(self.image_folder, post_id)\n",
    "        if os.path.exists(images_path):\n",
    "            image_files = os.listdir(images_path)\n",
    "            images = [Image.open(os.path.join(images_path, img)) for img in image_files]\n",
    "        else:\n",
    "            images = [Image.new(\"RGB\", (224, 224), \"black\")]\n",
    "            \n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "        images = [self.transform(img) for img in images]\n",
    "        images_tensor = torch.stack(images)\n",
    "        sample = {'text': inputs, 'images': images_tensor, 'labels': labels}\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform function for image processing (training)\n",
    "# Performing data augmentation by random resizing, cropping\n",
    "# and flipping images in order to artificially create new\n",
    "# image data per training epoch\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.255]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just normalization for validation\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.255]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create the dataset\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "\n",
    "train_dataset = MultiModalDataset(df_train, image_path, tokenizer, train_transform )\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True, prefetch_factor=2)\n",
    "\n",
    "val_dataset = MultiModalDataset(df_val, image_path, tokenizer, train_transform )\n",
    "validate_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True, prefetch_factor=2)\n",
    "\n",
    "test_dataset = MultiModalDataset(df_test, image_path, tokenizer, val_transform )\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True, prefetch_factor=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation\n",
    "- The model has the following major components\n",
    "    - Pretrained BERT model for getting embeddings for the title and a pretrained resnet for the image embeddings \n",
    "    - Performs transfer learning based on the pretrained models (BERT and RESNET)\n",
    "    - Tokenization of the title input using transformers\n",
    "    - Cross attention mechanism implemented with 4 attention heads (Increased the number of heads for improvement in performance)\n",
    "        - Each image attents to a title i.e if there are 5 images in the article, they will attent to the same title or text given by id \n",
    "        - For each of the image or if it exists for a certain post, feed in the images and compute cross attention for the image to title attention (each image attenting to the post)\n",
    "        - The attention query in this case is the text (title), key is the image embeddings and value is the image embeddings\n",
    "        - It is flexible enough to take in any number of images per post\n",
    "        - The cross attention output is then averaged \n",
    "     - Additional functionalities like adding batch normalization to the image model layers, ReLU activation function for non linearity and dropout \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, bert, resnet):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.resnet = resnet\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        \n",
    "        # Defining the attention mechanism for the model\n",
    "        self.image_to_title_attention = nn.MultiheadAttention(bert.config.hidden_size, num_heads=4)  # Increase num_heads\n",
    "        \n",
    "        self.linear = nn.Linear(2048, bert.config.hidden_size)\n",
    "        self.norm = nn.BatchNorm1d(bert.config.hidden_size)\n",
    "        self.relu = nn.ReLU()  # Add ReLU activation\n",
    "        self.hidden = nn.Linear(bert.config.hidden_size, bert.config.hidden_size)  # Add hidden layer\n",
    "        self.classifier = nn.Linear(bert.config.hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs, images):\n",
    "        # Process text input\n",
    "        text_output = self.bert(**inputs).last_hidden_state[:, 0, :]\n",
    "\n",
    "        # Process image input\n",
    "        img_embeddings = [self.resnet(img.unsqueeze(0)).view(-1) for img in images]\n",
    "        img_embeddings = torch.stack(img_embeddings)\n",
    "        img_embeddings = self.linear(img_embeddings)\n",
    "        img_embeddings = self.norm(img_embeddings)  # Apply batch normalization\n",
    "        img_embeddings = self.relu(img_embeddings)  # Apply ReLU activation\n",
    "\n",
    "        # Calculate attention between text and each image\n",
    "        attention_outputs = []\n",
    "        for img_emb in img_embeddings:\n",
    "            img_emb = img_emb.view(1, 1, 768)\n",
    "            # text_output.unsqueeze(1).shape (1, batch_size, hidden_size) => (1, 2, 768)\n",
    "            #img_emb.shape => (1, 1, hidden_size)\n",
    "            att_out, _ = self.image_to_title_attention(text_output.unsqueeze(1), img_emb, img_emb)\n",
    "            attention_outputs.append(att_out)\n",
    "\n",
    "        # Average attention outputs\n",
    "        attention_output = torch.stack(attention_outputs).mean(dim=0)\n",
    "\n",
    "        # Classifier\n",
    "        logits = self.hidden(attention_output.squeeze(1))  # Apply hidden layer\n",
    "        logits = self.drop(logits)  # Apply dropout to the hidden layer\n",
    "        logits = self.classifier(logits)\n",
    "        return self.sigmoid(logits)\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training Configuration\n",
    "- Computing the class weights for the dataset so that we put different weights on the classes. It will help in class balancing during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(dataframe):\n",
    "    \n",
    "    # Count labels per class / subtype of Fake News in training set split\n",
    "    # in sorted order 0, 1 and put into label_count list\n",
    "    label_count = [dataframe[\"labels\"].value_counts().sort_index(0)[0],\n",
    "                   dataframe[\"labels\"].value_counts().sort_index(0)[1]]\n",
    "\n",
    "    # Calculate weights per class by subtracting from 1 label_count per class divided\n",
    "    # by sum of all label_counts\n",
    "    class_weights = [1 - (x / sum(label_count)) for x in label_count]\n",
    "\n",
    "    class_weights = torch.FloatTensor(class_weights)\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Routine\n",
    "- Training routing for the model. Some of the additional techniques applied include clipping the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data_loader, criterion, optimizer, device, num_examples):\n",
    "    print(\"Training model in progress..\")\n",
    "    print(\"-\" * 15)\n",
    "    model = model.train()\n",
    "    train_losses = []\n",
    "    correct_preds = 0\n",
    "    for data in tqdm(data_loader):\n",
    "        inputs = {k: v.squeeze(1).to(device) for k, v in data['text'].items()}\n",
    "        images = data['images'].to(device)\n",
    "        labels = data['labels'].to(device)\n",
    "    \n",
    "        # Forward pass\n",
    "        outputs = model(inputs, images.squeeze(1))\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        train_loss = criterion(outputs.squeeze(), labels.float())\n",
    "        \n",
    "        correct_preds += torch.sum(preds == labels)\n",
    "        train_losses.append(train_loss.item())\n",
    "        train_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "    # Return train_acc and train_loss values\n",
    "    return correct_preds.double() / num_examples, np.mean(train_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, loss_function, device, num_examples):\n",
    "    print(\"validation of the model in progress...\")\n",
    "    print(\"-\" * 15)\n",
    "    model = model.eval()\n",
    "    val_losses = []\n",
    "    correct_preds = 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(data_loader):\n",
    "            # Move data to the device\n",
    "            inputs = {k: v.squeeze(1).to(device) for k, v in data['text'].items()}\n",
    "            images = data['images'].to(device)\n",
    "            labels = data['labels'].to(device)\n",
    "            \n",
    "            outputs = model(inputs, images.squeeze(1))\n",
    "            \n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            \n",
    "            val_loss = loss_function(outputs.squeeze(), labels.float())\n",
    "            correct_preds += torch.sum(preds == labels)\n",
    "            val_losses.append(val_loss.item())\n",
    "    return correct_preds.double() / num_examples, np.mean(val_losses)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7605, 0.2395])\n"
     ]
    }
   ],
   "source": [
    "# Calculate class weights on basis of training split dataframe and print weight tensor\n",
    "class_weights = get_class_weights(df_train)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiModalModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (resnet): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (drop): Dropout(p=0.3, inplace=False)\n",
       "  (image_to_title_attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (linear): Linear(in_features=2048, out_features=768, bias=True)\n",
       "  (norm): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (hidden): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MultiModalModel(bert, resnet)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 40\n",
    "# Initializing weighted Cross Entropy Loss function and assignment to device\n",
    "loss_function = nn.CrossEntropyLoss(weight=class_weights).to(device)\n",
    "# Set up the loss function and optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n",
      "Training model in progress..\n",
      "---------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51c34cace8b440d8bd04abaa4622912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "\n",
    "# Iteration times the total number of epochs\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    print(\"-\" * 10)\n",
    "\n",
    "    train_acc, train_loss = train_model(\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        loss_function,\n",
    "        optimizer,\n",
    "        device,\n",
    "        len(df_train)\n",
    "    )\n",
    "    \n",
    "\n",
    "    print(f\"Train loss {train_loss} | Accuracy {train_acc}\")\n",
    "    print()\n",
    "    val_acc, val_loss = evaluate_model(\n",
    "            model,\n",
    "            validate_dataloader,\n",
    "            loss_function,\n",
    "            device,\n",
    "            len(df_val)\n",
    "    )\n",
    "\n",
    "    print(f\"Val   loss {val_loss} | Accuracy {val_acc}\")\n",
    "    print()\n",
    "\n",
    "print()\n",
    "print(\"Completed Training!\")\n",
    "print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the output results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training and validation accuracy curves across the epochs\n",
    "plt.plot(train_acc, color=\"green\", label=\"Training Accuracy\")\n",
    "plt.plot(val_acc, color=\"red\", label=\"Validation Accuracy\")\n",
    "\n",
    "plt.title(\"Training History\")\n",
    "# Defining x- and y-axis labels\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss, color=\"blue\", label=\"Training Loss\")\n",
    "plt.plot(val_loss, color=\"orange\", label=\"Validation Loss\")\n",
    "plt.title(\"Training History\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, data_loader, loss_function, device, num_examples):\n",
    "    print(\"Testing model in progress...\")\n",
    "    print(\"-\" * 15)\n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    correct_preds = 0\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "    real_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(data_loader):\n",
    "            inputs = {k: v.squeeze(1).to(device) for k, v in data['text'].items()}\n",
    "            images = data['images'].to(device)\n",
    "            labels = data['labels'].to(device)\n",
    "            \n",
    "            outputs = model(inputs, images.squeeze(1))\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            test_loss = loss_function(outputs.squeeze(), labels.float())\n",
    "            correct_preds += torch.sum(preds == labels)\n",
    "            test_losses.append(test_loss.item())\n",
    "            predictions.extend(preds)\n",
    "            prediction_probs.extend(outputs)\n",
    "            real_labels.extend(labels)\n",
    "    test_acc = correct_preds.double() / num_examples\n",
    "    test_loss = np.mean(test_losses)\n",
    "    predictions = torch.stack(predictions)\n",
    "    prediction_probs = torch.stack(prediction_probs)\n",
    "    real_labels = torch.stack(real_labels)\n",
    "    \n",
    "    # Return test_acc, test_loss, predictions, prediction_probs, real_labels\n",
    "    return test_acc, test_loss, predictions, prediction_probs, real_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing model on test data split and initilaizing test values\n",
    "test_acc, test_loss, y_preds, y_prediction_probs, y_test = test_model(\n",
    "    model,\n",
    "    test_dataloader,\n",
    "    loss_function,\n",
    "    device,\n",
    "    len(df_test)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing model test accuracy\n",
    "print(f\"Model testing accuracy for classifier:  {test_acc*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score, roc_auc_score, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting classification report\n",
    "print(classification_report(y_test.cpu(), y_preds.cpu(), target_names=CLASS_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or individual evaluations of the results\n",
    "print(\"f1 score is: \", f1_score(y_test.cpu(), y_preds.cpu(), average='macro'))\n",
    "print(\"precision score is: \", precision_score(y_test.cpu(), y_preds.cpu(), average='macro'))\n",
    "print(\"recall score is: \", recall_score(y_test.cpu(), y_preds.cpu(), average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"auc score for the model: \", roc_auc_score(y_test.cpu(), 1 - y_prediction_probs.cpu()[:,1], multi_class=\"ovo\", average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(confusion_matrix):\n",
    "    hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Purples\")\n",
    "    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha=\"right\")\n",
    "    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha=\"right\")\n",
    "    # Set x- and y-axis labels\n",
    "    plt.ylabel(\"Fakeddit Dataset Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.title(\"Confusion Matrix for the model\")\n",
    "    plt.tight_layout\n",
    "    plt.show()\n",
    "\n",
    "# Initialize confusion_matrix with y_test (ground truth labels) and predicted labels\n",
    "cm = confusion_matrix(y_test.cpu(), y_preds.cpu())\n",
    "df_cm = pd.DataFrame(cm, index=CLASS_NAMES, columns=CLASS_NAMES)\n",
    "plot_confusion_matrix(df_cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting AUC and ROC score for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create ROC curve\n",
    "def plot_auc(y_test, y_pred_proba):\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test,  1 - y_pred_proba[:,1])\n",
    "    plt.plot(fpr,tpr)\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.title(\"ROC curve of the model\")\n",
    "    plt.tight_layout\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc(y_test.cpu(), y_prediction_probs.cpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
