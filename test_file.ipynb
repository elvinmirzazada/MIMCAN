{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Python Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T20:55:01.356658Z",
     "end_time": "2023-04-26T20:55:02.993494Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, cohen_kappa_score\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Import nn module for building stacked layers and optimizers\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.nn import functional as F\n",
    "# Import modules for dataset configuration and loading\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from transformers import AdamW\n",
    "\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "from PIL import Image, ImageFile, UnidentifiedImageError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T20:55:02.971390Z",
     "end_time": "2023-04-26T20:55:03.028553Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T20:55:02.975868Z",
     "end_time": "2023-04-26T20:55:03.379540Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "gossicop_fake = pd.read_csv('datasets/gossipcop_fake.csv')\n",
    "gossicop_real = pd.read_csv('datasets/gossipcop_real.csv')\n",
    "politifact_fake = pd.read_csv('datasets/politifact_fake.csv')\n",
    "politifact_real = pd.read_csv('datasets/politifact_real.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding labels to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T20:55:03.920571Z",
     "end_time": "2023-04-26T20:55:03.930613Z"
    }
   },
   "outputs": [],
   "source": [
    "gossicop_fake['labels']= 0\n",
    "gossicop_real['labels']= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### concatenating the dataset and shuffle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T20:55:04.454297Z",
     "end_time": "2023-04-26T20:55:04.477371Z"
    }
   },
   "outputs": [],
   "source": [
    "df_gossip = pd.concat([gossicop_real, gossicop_fake], ignore_index=True)\n",
    "df_gossip = shuffle(df_gossip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T20:55:04.639434Z",
     "end_time": "2023-04-26T20:55:04.646668Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['id', 'news_url', 'title', 'tweet_ids', 'labels'], dtype='object')"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gossip.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T20:55:04.797211Z",
     "end_time": "2023-04-26T20:55:04.803931Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gossip.title.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset after dropping not news url containing rows\n",
      "Fake dataset info: \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5067 entries, 0 to 5322\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         5067 non-null   object\n",
      " 1   news_url   5067 non-null   object\n",
      " 2   title      5067 non-null   object\n",
      " 3   tweet_ids  4898 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 197.9+ KB\n",
      "None\n",
      "Real dataset info: \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 16804 entries, 0 to 16816\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         16804 non-null  object\n",
      " 1   news_url   16804 non-null  object\n",
      " 2   title      16804 non-null  object\n",
      " 3   tweet_ids  15747 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 656.4+ KB\n",
      "None\n",
      "Final dataset description:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 21871 entries, 10600 to 20862\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         21871 non-null  object\n",
      " 1   news_url   21871 non-null  object\n",
      " 2   title      21871 non-null  object\n",
      " 3   tweet_ids  20645 non-null  object\n",
      " 4   labels     21871 non-null  int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 1.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import read_data\n",
    "df_gossip = read_data.load_dataset_gossipcop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:55:05.160320Z",
     "end_time": "2023-04-26T20:55:05.717795Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T20:55:05.714477Z",
     "end_time": "2023-04-26T20:55:05.726997Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                     id                                           news_url  \\\n10147  gossipcop-885134  https://www.tmz.com/2018/06/07/jeremy-meeks-me...   \n11083  gossipcop-893342  https://www.instyle.com/news/leonardo-dicaprio...   \n4769   gossipcop-911046  https://ew.com/tv/2018/02/07/law-order-svu-rau...   \n13563  gossipcop-911815  https://ew.com/tv/2018/10/09/this-is-us-three-...   \n243    gossipcop-848433  https://www.dailymail.co.uk/tvshowbiz/article-...   \n\n                                                   title  \\\n10147      Jeremy and Melissa Meeks' Divorce a Done Deal   \n11083  Leonardo DiCaprio's 43rd Birthday Bash Was a S...   \n4769   Raúl Esparza exits Law & Order: SVU after six ...   \n13563  This Is Us producers on three-Kate scene, Jack...   \n243    Sharon Stone, 60, shares very rare photo with ...   \n\n                                               tweet_ids  labels  \n10147  915995914329743360\\t915996984980443136\\t916003...       0  \n11083  929926564996714496\\t929928091702583296\\t929928...       0  \n4769   961445047517569025\\t961455185854803969\\t961458...       0  \n13563  960682277289242629\\t960682516935036928\\t960682...       0  \n243    860140423016177667\\t860140790751916033\\t860140...       0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>news_url</th>\n      <th>title</th>\n      <th>tweet_ids</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10147</th>\n      <td>gossipcop-885134</td>\n      <td>https://www.tmz.com/2018/06/07/jeremy-meeks-me...</td>\n      <td>Jeremy and Melissa Meeks' Divorce a Done Deal</td>\n      <td>915995914329743360\\t915996984980443136\\t916003...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11083</th>\n      <td>gossipcop-893342</td>\n      <td>https://www.instyle.com/news/leonardo-dicaprio...</td>\n      <td>Leonardo DiCaprio's 43rd Birthday Bash Was a S...</td>\n      <td>929926564996714496\\t929928091702583296\\t929928...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4769</th>\n      <td>gossipcop-911046</td>\n      <td>https://ew.com/tv/2018/02/07/law-order-svu-rau...</td>\n      <td>Raúl Esparza exits Law &amp; Order: SVU after six ...</td>\n      <td>961445047517569025\\t961455185854803969\\t961458...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13563</th>\n      <td>gossipcop-911815</td>\n      <td>https://ew.com/tv/2018/10/09/this-is-us-three-...</td>\n      <td>This Is Us producers on three-Kate scene, Jack...</td>\n      <td>960682277289242629\\t960682516935036928\\t960682...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>243</th>\n      <td>gossipcop-848433</td>\n      <td>https://www.dailymail.co.uk/tvshowbiz/article-...</td>\n      <td>Sharon Stone, 60, shares very rare photo with ...</td>\n      <td>860140423016177667\\t860140790751916033\\t860140...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gossip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T20:55:05.827722Z",
     "end_time": "2023-04-26T20:55:05.838568Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "3549"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_gossip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T20:56:20.973750Z",
     "end_time": "2023-04-26T20:56:21.018714Z"
    }
   },
   "outputs": [],
   "source": [
    "test = df_gossip.copy().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T20:56:21.503167Z",
     "end_time": "2023-04-26T20:56:21.516795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T20:56:22.659192Z",
     "end_time": "2023-04-26T20:56:22.666589Z"
    }
   },
   "outputs": [],
   "source": [
    "image_path = 'gossipcop_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T20:56:22.795149Z",
     "end_time": "2023-04-26T20:56:22.841644Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from transformers import BertModel, BertTokenizer, DistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T20:56:23.099726Z",
     "end_time": "2023-04-26T20:56:24.913337Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained models\n",
    "resnet = models.resnet50(pretrained=True) # resnet18\n",
    "resnet = nn.Sequential(*list(resnet.children())[:-1])  # Remove the last layer to get embeddings\n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning device to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T20:56:24.917374Z",
     "end_time": "2023-04-26T20:56:24.925322Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU is allocated.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if str(device) == \"cpu\":\n",
    "    print(\"CPU is allocated.\")\n",
    "else:\n",
    "    print(\"GPU is allocated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T20:56:24.929686Z",
     "end_time": "2023-04-26T20:56:24.967461Z"
    }
   },
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class MultiModalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, image_folder, tokenizer, transform):\n",
    "        self.df = df\n",
    "        self.image_folder = image_folder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        #pdb.set_trace()\n",
    "        text, post_id, labels = row[\"title\"], row[\"id\"], row[\"labels\"]\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "        images_path = os.path.join(self.image_folder, post_id)\n",
    "        images = []\n",
    "        if os.path.exists(images_path):\n",
    "            image_files = os.listdir(images_path)\n",
    "\n",
    "            for img in image_files:\n",
    "                try:\n",
    "                    image = Image.open(os.path.join(images_path, img))\n",
    "                    if image.mode != 'RGB':\n",
    "                        image = image.convert('RGB')\n",
    "                    images.append(image)\n",
    "                # Handling Exception and randomly initializing pixels\n",
    "                except Exception:\n",
    "                    images.append(Image.new(\"RGB\", (224, 224), \"black\"))\n",
    "        else:\n",
    "            images = [Image.new(\"RGB\", (224, 224), \"black\")]\n",
    "            \n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "        images = [self.transform(img) for img in images]\n",
    "        images_tensor = torch.stack(images)\n",
    "        sample = {'text': inputs, 'images': images_tensor, 'labels': labels}\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T20:56:24.943011Z",
     "end_time": "2023-04-26T20:56:24.968555Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.255]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Just normalization for validation\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.255]\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T20:57:05.297832Z",
     "end_time": "2023-04-26T20:57:05.340974Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "dataset = MultiModalDataset(test, image_path, tokenizer, transform )\n",
    "# Create the data loader\n",
    "train_dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T20:57:06.113619Z",
     "end_time": "2023-04-26T20:57:06.169933Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, bert, resnet):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.resnet = resnet\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        \n",
    "        # Defining the attention mechanism for the model\n",
    "        self.image_to_title_attention = nn.MultiheadAttention(bert.config.hidden_size, num_heads=4)  # Increase num_heads\n",
    "        \n",
    "        self.linear = nn.Linear(2048, bert.config.hidden_size)\n",
    "        self.norm = nn.BatchNorm1d(bert.config.hidden_size)\n",
    "        self.relu = nn.ReLU()  # Add ReLU activation\n",
    "        self.hidden = nn.Linear(bert.config.hidden_size, bert.config.hidden_size)  # Add hidden layer\n",
    "        self.classifier = nn.Linear(bert.config.hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs, images):\n",
    "        # Process text input\n",
    "        text_output = self.bert(**inputs).last_hidden_state[:, 0, :]\n",
    "\n",
    "        # Process image input\n",
    "        img_embeddings = [self.resnet(img.unsqueeze(0)).view(-1) for img in images]\n",
    "        img_embeddings = torch.stack(img_embeddings)\n",
    "        print(img_embeddings.shape)\n",
    "        img_embeddings = self.linear(img_embeddings)\n",
    "        img_embeddings = self.norm(img_embeddings)  # Apply batch normalization\n",
    "        img_embeddings = self.relu(img_embeddings)  # Apply ReLU activation\n",
    "\n",
    "        # Calculate attention between text and each image\n",
    "        attention_outputs = []\n",
    "        for img_emb in img_embeddings:\n",
    "            img_emb = img_emb.view(1, 1, 768)\n",
    "            # text_output.unsqueeze(1).shape (1, batch_size, hidden_size) => (1, 2, 768)\n",
    "            #img_emb.shape => (1, 1, hidden_size)\n",
    "            att_out, _ = self.image_to_title_attention(text_output.unsqueeze(1), img_emb, img_emb)\n",
    "            attention_outputs.append(att_out)\n",
    "\n",
    "        # Average attention outputs\n",
    "        attention_output = torch.stack(attention_outputs).mean(dim=0)\n",
    "\n",
    "        # Classifier\n",
    "        logits = self.hidden(attention_output.squeeze(1))  # Apply hidden layer\n",
    "        logits = self.drop(logits)  # Apply dropout to the hidden layer\n",
    "        logits = self.classifier(logits)\n",
    "        return self.sigmoid(logits)\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T20:57:06.478936Z",
     "end_time": "2023-04-26T20:57:06.531665Z"
    }
   },
   "outputs": [],
   "source": [
    "model = MultiModalModel(bert, resnet)\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set up the loss function and optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-25T23:29:44.632713Z",
     "end_time": "2023-04-25T23:34:20.520121Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f20fb4ab60e40b28eb6d5e8ad37de25"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 3, 3, 224, 224]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[53], line 17\u001B[0m\n\u001B[1;32m     14\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimages\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs\u001B[38;5;241m.\u001B[39msqueeze(), labels\u001B[38;5;241m.\u001B[39mfloat())\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Backward pass and optimization\u001B[39;00m\n",
      "File \u001B[0;32m~/elvin/UT Courses/THESIS/SOURCE CODES/MIMCAN/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[51], line 26\u001B[0m, in \u001B[0;36mMultiModalModel.forward\u001B[0;34m(self, inputs, images)\u001B[0m\n\u001B[1;32m     23\u001B[0m text_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbert(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs)\u001B[38;5;241m.\u001B[39mlast_hidden_state[:, \u001B[38;5;241m0\u001B[39m, :]\n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# Process image input\u001B[39;00m\n\u001B[0;32m---> 26\u001B[0m img_embeddings \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresnet(img\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m))\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m img \u001B[38;5;129;01min\u001B[39;00m images]\n\u001B[1;32m     27\u001B[0m img_embeddings \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack(img_embeddings)\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28mprint\u001B[39m(img_embeddings\u001B[38;5;241m.\u001B[39mshape)\n",
      "Cell \u001B[0;32mIn[51], line 26\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     23\u001B[0m text_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbert(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs)\u001B[38;5;241m.\u001B[39mlast_hidden_state[:, \u001B[38;5;241m0\u001B[39m, :]\n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# Process image input\u001B[39;00m\n\u001B[0;32m---> 26\u001B[0m img_embeddings \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m img \u001B[38;5;129;01min\u001B[39;00m images]\n\u001B[1;32m     27\u001B[0m img_embeddings \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack(img_embeddings)\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28mprint\u001B[39m(img_embeddings\u001B[38;5;241m.\u001B[39mshape)\n",
      "File \u001B[0;32m~/elvin/UT Courses/THESIS/SOURCE CODES/MIMCAN/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/elvin/UT Courses/THESIS/SOURCE CODES/MIMCAN/env/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/elvin/UT Courses/THESIS/SOURCE CODES/MIMCAN/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/elvin/UT Courses/THESIS/SOURCE CODES/MIMCAN/env/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    462\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 463\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/elvin/UT Courses/THESIS/SOURCE CODES/MIMCAN/env/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[1;32m    457\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[1;32m    458\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[0;32m--> 459\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    460\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 3, 3, 224, 224]"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for data in tqdm(train_dataloader):\n",
    "        # Move data to the device\n",
    "        inputs = {k: v.squeeze(1).to(device) for k, v in data['text'].items()}\n",
    "        #inputs = data['text'].to(device)\n",
    "        #images = torch.stack(data['images']).to(device)\n",
    "        images = data['images'].to(device)\n",
    "        labels = data['labels'].to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs, images.squeeze(1))\n",
    "        loss = criterion(outputs.squeeze(), labels.float())\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    avg_loss = running_loss / len(train_dataloader)\n",
    "    print(f\"Epoch: {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T20:56:45.545490Z",
     "end_time": "2023-04-26T20:56:45.597302Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data_loader, loss_function, optimizer, device, num_examples):\n",
    "    print(\"Training model in progress..\")\n",
    "    print(\"-\" * 15)\n",
    "    model = model.train()\n",
    "    train_losses = []\n",
    "    correct_preds = 0\n",
    "    for data in tqdm(data_loader):\n",
    "        input_ids = data[\"input_ids\"].to(device)\n",
    "        attention_mask = data[\"attention_mask\"].to(device)\n",
    "        images = data[\"image\"].to(device)\n",
    "        labels = data[\"label\"].to(device)\n",
    "  \n",
    "        outputs = model(\n",
    "                title_input_ids = input_ids,\n",
    "                title_attention_mask = attention_mask,\n",
    "                image = images\n",
    "        )\n",
    "        \n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        train_loss = loss_function(outputs, labels)\n",
    "        correct_preds += torch.sum(preds == labels)\n",
    "        train_losses.append(train_loss.item())\n",
    "        train_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "    # Return train_acc and train_loss values\n",
    "    return correct_preds.double() / num_examples, np.mean(train_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, loss_function, device, num_examples):\n",
    "    print(\"validation of the model in progress...\")\n",
    "    print(\"-\" * 15)\n",
    "    model = model.eval()\n",
    "    val_losses = []\n",
    "    correct_preds = 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(data_loader):\n",
    "            input_ids = data[\"input_ids\"].to(device)\n",
    "            attention_mask = data[\"attention_mask\"].to(device)\n",
    "            images = data[\"image\"].to(device)\n",
    "            labels = data[\"label\"].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                    title_input_ids = input_ids,\n",
    "                    title_attention_mask = attention_mask,\n",
    "                    image = images\n",
    "                 )\n",
    "            \n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            \n",
    "            val_loss = loss_function(outputs, labels)\n",
    "            correct_preds += torch.sum(preds == labels)\n",
    "            val_losses.append(val_loss.item())\n",
    "    return correct_preds.double() / num_examples, np.mean(val_losses)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0\n",
    "\n",
    "# Iteration times the total number of epochs\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    print(\"-\" * 10)\n",
    "\n",
    "    train_acc, train_loss = train_model(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        loss_function,\n",
    "        optimizer,\n",
    "        device,\n",
    "        len(df_train)\n",
    "    )\n",
    "    \n",
    "\n",
    "    print(f\"Train loss {train_loss} | Accuracy {train_acc}\")\n",
    "    print()\n",
    "    val_acc, val_loss = evaluate_model(\n",
    "            model,\n",
    "            validate_data_loader,\n",
    "            loss_function,\n",
    "            device,\n",
    "            len(df_val)\n",
    "    )\n",
    "\n",
    "    print(f\"Val   loss {val_loss} | Accuracy {val_acc}\")\n",
    "    print()\n",
    "\n",
    "print()\n",
    "print(\"Completed Training!\")\n",
    "print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
